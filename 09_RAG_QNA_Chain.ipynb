{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVuKUVPrn6kv",
        "outputId": "22285a38-1770-471e-f624-cd44cf95baf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.1/417.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.5/254.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.1/589.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m966.2/966.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "argilla 1.29.0 requires typer<0.10.0,>=0.6.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain==0.0.219 openai==0.27.8\n",
        "\n",
        "# !pip install -q transformers==4.29.2 sentencepiece==0.1.99 accelerate==0.19.0 bitsandbytes==0.39.0\n",
        "\n",
        "!pip install -q python-dotenv==1.0.0\n",
        "\n",
        "!pip install -q pandas==1.5.3\n",
        "!pip install -q unstructured==0.7.12\n",
        "!pip install -q wikipedia==1.4.0\n",
        "!pip install -q pypdf==3.12.0\n",
        "!pip install -q jq==1.4.1\n",
        "!pip install -q nltk==3.8.1\n",
        "!pip install -q tiktoken==0.4.0\n",
        "!pip install -q transformers==4.29.2 sentencepiece==0.1.99\n",
        "!pip install -q sentence-transformers==2.2.2\n",
        "!pip install -q cohere==4.11.2\n",
        "!pip install -q faiss-cpu==1.7.4\n",
        "!pip install -q chromadb==0.3.26"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import dotenv_values\n",
        "env_values = dotenv_values(\"./app.env\")\n",
        "openai_api_key = env_values['OPEN_API_KEY']"
      ],
      "metadata": {
        "id": "riAXEoEsoTUE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.9, openai_api_key=openai_api_key)\n",
        "llm2 = OpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.9, openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzCsSG26sZdy",
        "outputId": "f319d0bb-87f8-4197-86c7-155eddbb5961"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:189: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:769: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WikipediaLoader\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n"
      ],
      "metadata": {
        "id": "aCHC2gA0siLT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_1 = \"Nikola Tesla\"\n",
        "query_2 = \"Thomas Edison\"\n",
        "\n",
        "text_document_1 = WikipediaLoader(query=query_1,doc_content_chars_max=20000,load_max_docs = 1).load()\n",
        "text_document_2 = WikipediaLoader(query=query_2,doc_content_chars_max=20000,load_max_docs= 1).load()"
      ],
      "metadata": {
        "id": "InBiMA9GsxcE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_document_2 =  text_document_2[0].page_content\n",
        "text_document_1 =  text_document_1[0].page_content"
      ],
      "metadata": {
        "id": "ZkNPvRsBuVLv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documnets = [text_document_1, text_document_2]\n",
        "metadata = [{\"document\":query_1},{\"document\" : query_2}]"
      ],
      "metadata": {
        "id": "OyhgnRRttGpk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import NLTKTextSplitter\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AW1q9vmtV0B",
        "outputId": "ba5c7e04-e20f-4cc7-c046-1428d5af0930"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text =  NLTKTextSplitter(chunk_size=300, chunk_overlap=20)\n",
        "text = text.create_documents(documnets, metadatas=metadata )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0UHBLEetstE",
        "outputId": "439bc965-fcee-46ff-a278-4c048c974024"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 315, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 316, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 369, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 504, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 307, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 311, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 328, which is longer than the specified 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klOf0e4xt35G",
        "outputId": "eca93c35-0ad9-42d2-bffe-3006e3fb04ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "175"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "HyU7dqbouq4K"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#embeddings =  embeddings.embed_documents(text)\n",
        "\n",
        "vector =  Chroma.from_documents(text, embeddings,persist_directory=\"./chroma\")"
      ],
      "metadata": {
        "id": "aTTNhhKtu6dL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Suff QNA"
      ],
      "metadata": {
        "id": "YEQ7vB05v31b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "-RMXI7Qmu-Ai"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "promp = \"\"\"\n",
        "Answer the following question perfectly using provided context ,\n",
        "If you can't find an answer of the question just diplay  I am stupid,\n",
        "\"#### Context\" ,\n",
        "{context},\n",
        "\"\"\n",
        "\"#### Question\"\n",
        "{question},\n",
        "\"\"\n",
        "\"####\" Answer:\n",
        "\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sFfB2FWqwKIF"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = PromptTemplate(template=promp, input_variables=[\"context\", \"question\"])"
      ],
      "metadata": {
        "id": "CyJNGpzvwp9U"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"when and where the father of tesla died\""
      ],
      "metadata": {
        "id": "jbtflKrZwtYH"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar = vector.similarity_search(query = question , k = 4 )"
      ],
      "metadata": {
        "id": "3b7fG_CpxPF5"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = chain({\"input_documents\": similar, \"question\": question}, return_only_outputs=True)"
      ],
      "metadata": {
        "id": "kp89RRJBw1rM"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0GcE9o4w6Ei",
        "outputId": "d45fe8fb-3fb2-4d08-e7b6-a1c9e0b9361b"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'output_text': \"Tesla's father, Milutin Tesla, died on 17 April 1879 in Smiljan.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Map reduce"
      ],
      "metadata": {
        "id": "HatOMbATw82H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "promp_2 = \"\"\"\n",
        "\"Given intermediate contexts for a question, generate a final answer.\",\n",
        "\"If the answer is not contained in the intermediate contexts, say 'NO ANSWER IS AVAILABLE'\",\n",
        "\"#### summaries\" ,\n",
        "{summaries},\n",
        "\"\",\n",
        "\"#### Question\"\n",
        "{question},\n",
        "\"\",\n",
        "\"#### Final Answer \",\n",
        "\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9mZWxsFrw7Rs"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_2  = PromptTemplate(\n",
        "    template=promp_2,\n",
        "    input_variables=['summaries', 'question'])"
      ],
      "metadata": {
        "id": "225mOmo9x0ae"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_reduce_chain = load_qa_chain(llm, chain_type=\"map_reduce\",\n",
        "                                 return_intermediate_steps=True,\n",
        "                                 question_prompt=temp,\n",
        "                                 combine_prompt=temp_2)"
      ],
      "metadata": {
        "id": "Kd1vL04Jx2JX"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_answer = map_reduce_chain({\"input_documents\": similar, \"question\": question}, return_only_outputs=True)\n"
      ],
      "metadata": {
        "id": "AAeJiO-3yOj3"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLfGFwuo0-rB",
        "outputId": "12f168b3-40a4-4b09-a741-bb5fbf72e6b3"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intermediate_steps': [\"Tesla's father died on 17 April 1879 in Gospić.\",\n",
              "  'His father, Milutin Tesla, died in 1879. The location of his death is not provided in the context.',\n",
              "  \"Tesla's father died in New York City in January 1943.\",\n",
              "  'I am stupid'],\n",
              " 'output_text': 'NO ANSWER IS AVAILABLE'}"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refine"
      ],
      "metadata": {
        "id": "SfVwE__D18Gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_qna_template = \"\\n\".join([\n",
        "    \"Answer the following question using the provided text only.\",\n",
        "    \"If answer is not available. Say 'No answer for this context'\",\n",
        "    \"### Context:\",\n",
        "    \"{context_str}\",\n",
        "    \"\",\n",
        "    \"### Question:\",\n",
        "    \"{question}\",\n",
        "    \"### Answer:\",\n",
        "])\n",
        "\n",
        "initial_qna_prompt = PromptTemplate(\n",
        "    template=initial_qna_template,\n",
        "    input_variables=['context_str', 'question']\n",
        ")"
      ],
      "metadata": {
        "id": "Y9hKGABI2-at"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refine_qna_template = \"\\n\".join([\n",
        "    \"Refine the existing answer, if required, with the following context.\",\n",
        "    \"If answer is not available. Say 'No answer for this context'\",\n",
        "    \"### Context\",\n",
        "    \"{context_str}\",\n",
        "    \"\",\n",
        "    \"### Existing Answer:\",\n",
        "    \"{existing_answer}\",\n",
        "    \"\",\n",
        "    \"### Question:\",\n",
        "    \"{question}\",\n",
        "    \"\",\n",
        "    \"### Refined Answer:\",\n",
        "])\n",
        "\n",
        "refine_qna_prompt = PromptTemplate(\n",
        "    template=refine_qna_template,\n",
        "    input_variables=['context_str', 'existing_answer', 'question']\n",
        ")"
      ],
      "metadata": {
        "id": "f9nBU-yA2S0u"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refine_chain = load_qa_chain(\n",
        "    llm,\n",
        "    chain_type=\"refine\",\n",
        "    question_prompt=initial_qna_prompt,\n",
        "    refine_prompt=refine_qna_prompt,\n",
        "    return_intermediate_steps=True,\n",
        ")"
      ],
      "metadata": {
        "id": "ox9hiskF17hW"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_answer = refine_chain({\"input_documents\": similar, \"question\": question}, return_only_outputs=True)\n",
        "final_answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pXtNu2q10oa",
        "outputId": "9549e808-851f-4912-ff34-10631f48715d"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intermediate_steps': [\"Tesla's father died on 17 April 1879 in Gospić.\",\n",
              "  \"Tesla's father, Milutin Tesla, died on April 17, 1879, in Gospić.\",\n",
              "  \"Tesla's father, Milutin Tesla, died on April 17, 1879, in Gospić, Croatia.\",\n",
              "  \"Tesla's father, Milutin Tesla, died on April 17, 1879, in Gospić, Croatia, after Tesla finished a four-year term in three years and graduated in 1873.\"],\n",
              " 'output_text': \"Tesla's father, Milutin Tesla, died on April 17, 1879, in Gospić, Croatia, after Tesla finished a four-year term in three years and graduated in 1873.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VKSRwa_d3Lag"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}